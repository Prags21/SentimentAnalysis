{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Welcome To Colaboratory",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prags21/SentimentAnalysis/blob/master/geopy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0ygD3pubC6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "#Module to handle regular expressions\n",
        "import re\n",
        "#manage files\n",
        "\n",
        "import os\n",
        "#Library for emoji\n",
        "#import emoji\n",
        "#Import pandas and numpy to handle data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#import libraries for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "\n",
        "#Import nltk to check english lexicon\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from nltk.corpus import (\n",
        "    wordnet,\n",
        "    stopwords\n",
        ")\n",
        "\n",
        "#import libraries for tokenization and ML\n",
        "import json;\n",
        "import keras;\n",
        "import keras.preprocessing.text as kpt;\n",
        "#from keras.preprocessing.text import Tokenizer;\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.feature_extraction.text import (\n",
        "    CountVectorizer,\n",
        "    TfidfVectorizer\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "\n",
        "#Import all libraries for creating a deep neural network\n",
        "#Sequential is the standard type of neural network with stackable layers\n",
        "from keras.models import (\n",
        "    Sequential,\n",
        "    model_from_json\n",
        ")\n",
        "#Dense: Standard layers with every node connected, dropout: avoids overfitting\n",
        "from keras.layers import Dense, Dropout, Activation;\n",
        "table=pd.read_csv('RawResult.csv',delimiter = ',',encoding = \"utf-8\")\n",
        "stop = stopwords.words('english')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfB1txJvZN0r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "48f4cced-536e-43cd-e104-bf4ef79347dd"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 322
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kypJGh1nbfKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preprocess text in tweets by removing links, @UserNames, blank spaces, etc.\n",
        "def preprocessing_text(table):\n",
        "    print(table['Tweet Text'][10]) \n",
        "\n",
        "    #put everythin in lowercase\n",
        "    table['Tweet Text'] = table['Tweet Text'].str.lower()\n",
        "  \n",
        "    #removing unicode characters  \n",
        "    for ind,item in enumerate(table['Tweet Text']):\n",
        "        a=''\n",
        "        a= bytes(item, \"utf-8\").decode(\"unicode_escape\")\n",
        "        table.at[ind,'Tweet Text'] = re.sub(r'[^\\x00-\\x7f]',r' ',a) \n",
        "\n",
        "    table['Tweet Text'] = table['Tweet Text'].apply(lambda x: re.compile('\\#[A-Za-z0-9]+\\W').sub('', re.compile('rt @').sub('@', x).strip()))\n",
        "\n",
        "    #Replace rt indicating that was a retweet\n",
        "    #table['Tweet Text'] = table['Tweet Text'].str.replace(r'[^rt @\\w+\\W]', '')\n",
        "\n",
        "    #Replace occurences of mentioning @UserNames\n",
        "    table['Tweet Text'] = table['Tweet Text'].replace(r'@\\w+', '', regex=True)\n",
        "    #Replace links contained in the tweet\n",
        "    table['Tweet Text'] = table['Tweet Text'].replace(r'http\\S+', '', regex=True)\n",
        "    table['Tweet Text'] = table['Tweet Text'].replace(r'www.[^ ]+', '', regex=True)\n",
        "    #remove numbers\n",
        "    table['Tweet Text'] = table['Tweet Text'].replace(r'[0-9]+', '', regex=True)\n",
        "    #replace special characters and puntuation marks\n",
        "    table['Tweet Text'] = table['Tweet Text'].replace(r'[!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~]', '', regex=True)\n",
        "   \n",
        "    for ind,item in enumerate(table['User Location']):\n",
        "        y=''\n",
        "        a= bytes(item, \"utf-8\").decode(\"unicode_escape\")\n",
        "        cleaned_loc = re.sub(r'[^\\x00-\\x7f]',r' ',a) \n",
        "        y = cleaned_loc.replace('b\\'', r'')\n",
        "        z= y.replace('\\'', ' ')\n",
        "        table.at[ind,'User Location'] =z\n",
        "    for ind,item in enumerate(table['Phone Type']):\n",
        "        a=''\n",
        "        a= bytes(item, \"utf-8\").decode(\"unicode_escape\")\n",
        "        table.at[ind,'Phone Type'] = re.sub(r'[^\\x00-\\x7f]',r' ',a) \n",
        "    #table['Phone Type']=table['Phone Type'].decode(\"utf-8\")\n",
        "\n",
        "    #print(table['Tweet Text'][10]) \n",
        "    print(table['User Location'][10]) \n",
        "    #print(table['Phone Type'][10]) \n",
        "\n",
        "    return table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPxrs_GDfubK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#table=preprocessing_text(table)  \n",
        "#table['Tweet Text'] = table['Tweet Text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "#print(TextBlob(train['Tweet text'][0]).ngrams(2))\n",
        "\n",
        "\n",
        "#print(TextBlob(table['Tweet Text'][1]).words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBqWb5WxcPVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Replace elongated words by identifying those repeated characters and then remove them and compare the new word with the english lexicon\n",
        "def in_dict(word):\n",
        "    if wordnet.synsets(word):\n",
        "        #if the word is in the dictionary, we'll return True\n",
        "        return True\n",
        "\n",
        "def replace_elongated_word(word):\n",
        "    regex = r'(\\w*)(\\w+)\\2(\\w*)'\n",
        "    repl = r'\\1\\2\\3'    \n",
        "    if in_dict(word):\n",
        "        return word\n",
        "    new_word = re.sub(regex, repl, word)\n",
        "    if new_word != word:\n",
        "        return replace_elongated_word(new_word)\n",
        "    else:\n",
        "        return new_word\n",
        "\n",
        "def detect_elongated_words(row):\n",
        "    regexrep = r'(\\w*)(\\w+)(\\2)(\\w*)'\n",
        "    words = [''.join(i) for i in re.findall(regexrep, row)]\n",
        "    for word in words:\n",
        "        if not in_dict(word):\n",
        "            row = re.sub(word, replace_elongated_word(word), row)\n",
        "    return row"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnvesvUOcXcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stop_words(table):\n",
        "    #We need to remove the stop words\n",
        "    stop_words_list = stopwords.words('english')\n",
        "    table['Tweet Text'] = table['Tweet Text'].str.lower()\n",
        "    table['Tweet Text'] = table['Tweet Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words_list)]))\n",
        "    return table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZeCvS80cb3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def replace_antonyms(word):\n",
        "    #We get all the lemma for the word\n",
        "    for syn in wordnet.synsets(word): \n",
        "        for lemma in syn.lemmas(): \n",
        "            #if the lemma is an antonyms of the word\n",
        "            if lemma.antonyms(): \n",
        "                #we return the antonym\n",
        "                return lemma.antonyms()[0].name()\n",
        "    return word\n",
        "            \n",
        "def handling_negation(row):\n",
        "    #Tokenize the row\n",
        "    words = word_tokenize(row)\n",
        "    speach_tags = ['JJ', 'JJR', 'JJS', 'NN', 'VB', 'VBD', 'VBG', 'VBN', 'VBP']\n",
        "    #We obtain the type of words that we have in the text, we use the pos_tag function\n",
        "    tags = nltk.pos_tag(words)\n",
        "    #Now we ask if we found a negation in the words\n",
        "    tags_2 = ''\n",
        "    if \"n't\" in words and \"not\" in words:\n",
        "        tags_2 = tags[min(words.index(\"n't\"), words.index(\"not\")):]\n",
        "        words_2 = words[min(words.index(\"n't\"), words.index(\"not\")):]\n",
        "        words = words[:(min(words.index(\"n't\"), words.index(\"not\")))+1]\n",
        "    elif \"n't\" in words:\n",
        "        tags_2 = tags[words.index(\"n't\"):]\n",
        "        words_2 = words[words.index(\"n't\"):] \n",
        "        words = words[:words.index(\"n't\")+1]\n",
        "    elif \"not\" in words:\n",
        "        tags_2 = tags[words.index(\"not\"):]\n",
        "        words_2 = words[words.index(\"not\"):]\n",
        "        words = words[:words.index(\"not\")+1] \n",
        "        \n",
        "    for index, word_tag in enumerate(tags_2):\n",
        "        if word_tag[1] in speach_tags:\n",
        "            words = words+[replace_antonyms(word_tag[0])]+words_2[index+2:]\n",
        "            break\n",
        "            \n",
        "    return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5ld02CFchFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleaning_table(table):\n",
        "    #This function will process all the required cleaning for the text in our tweets\n",
        "    table = preprocessing_text(table)\n",
        "    table['Tweet Text'] = table['Tweet Text'].apply(lambda x: detect_elongated_words(x))\n",
        "    table['Tweet Text'] = table['Tweet Text'].apply(lambda x: handling_negation(x))\n",
        "    table = stop_words(table)\n",
        "    return table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqL2kUM7tMpu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "ad99cabf-d64c-485b-d656-d086a9fe7694"
      },
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "table=cleaning_table(table)  \n",
        "\n"
      ],
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b\"It's all tied up! Let's get some more votes in so we can really see where everyone stands on the dangers of #Artificial #Intelligence #WritingCommunity #AI #Augmentedintelligence #vs #ArtificialIntelligence https://t.co/DlxeVgGzKm\"\n",
            "Columbus, OH \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMvQCWBnomGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "geolocator = Nominatim(user_agent=\"specify_your_app_name_here\")\n",
        "state=[]\n",
        "coordinates=[]\n",
        "country=[]\n",
        "for ind,item in enumerate(table['User Location']):\n",
        "    location=''\n",
        "    location = geolocator.geocode(item, addressdetails=True,exactly_one=True, timeout=60)\n",
        "    if location is not None:\n",
        "        #state.append(location.raw['address']['state'])\n",
        "        if 'lon' in location.raw:\n",
        "          coordinates.append((location.raw['lon'],location.raw['lat']))\n",
        "        if 'country' in location.raw['address']:\n",
        "          country.append(location.raw['address']['country'])\n",
        "          print(location.raw['address']['country'])\n",
        "           \n",
        "print(country)\n",
        "Counter(country)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFY29avnupIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import folium\n",
        " \n",
        "# Load the shape of the zone (US states)\n",
        "# Find the original file here: https://github.com/python-visualization/folium/tree/master/examples/data\n",
        "# You have to download this file and set the directory where you saved it\n",
        "state_geo = os.path.join('/Users/y.holtz/Desktop/', 'us-states.json')\n",
        " \n",
        "# Load the unemployment value of each state\n",
        "# Find the original file here: https://github.com/python-visualization/folium/tree/master/examples/data\n",
        "state_data = pd.read_csv('trial.csv')\n",
        " \n",
        "# Initialize the map:\n",
        "m = folium.Map(location=[37, -102], zoom_start=5)\n",
        " \n",
        "# Add the color for the chloropleth:\n",
        "m.choropleth(\n",
        " geo_data='states.json',\n",
        " name='choropleth',\n",
        " data=state_data,\n",
        " columns=['State', 'Tweets'],\n",
        " key_on='feature.id',\n",
        " fill_color='YlGn',\n",
        " fill_opacity=0.7,\n",
        " line_opacity=0.2,\n",
        " legend_name='Tweets distribution (%)'\n",
        ")\n",
        "folium.LayerControl().add_to(m)\n",
        "m\n",
        "# Save to html\n",
        "m.save('output.html')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6WzBWQSc0DP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Vectorization for Data Visualization\n",
        "def vectorization(table):\n",
        "    #CountVectorizer will convert a collection of text documents to a matrix of token counts\n",
        "    #Produces a sparse representation of the counts \n",
        "    #Initialize\n",
        "    vector = CountVectorizer()\n",
        "    #We fit and transform the vector created\n",
        "    frequency_matrix = vector.fit_transform(table['Tweet Text'])\n",
        "    #Sum all the frequencies for each word\n",
        "    sum_frequencies = np.sum(frequency_matrix, axis=0)\n",
        "    #Now we use squeeze to remove single-dimensional entries from the shape of an array that we got from applying np.asarray to\n",
        "    #the sum of frequencies.\n",
        "    frequency = np.squeeze(np.asarray(sum_frequencies))\n",
        "    #Now we get into a dataframe all the frequencies and the words that they correspond to\n",
        "    frequency_df = pd.DataFrame([frequency], columns=vector.get_feature_names())#.transpose()\n",
        "    list_words=[]\n",
        "    for name in frequency_df.columns:\n",
        "      dict={}\n",
        "      dict['name']=name\n",
        "      dict['frequency']=frequency_df[name][0]\n",
        "      list_words.append(dict);\n",
        "    return list_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCjgvfmndF21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def word_cloud(tweets):\n",
        "    \n",
        "    #We get the directory that we are working on\n",
        "    file = os.getcwd()\n",
        "    #We read the mask image into a numpy array\n",
        "    #Now we store the tweets into a series to be able to process \n",
        "    #tweets_list = pd.Series([t for t in tweet_table.tweet]).str.cat(sep=' ')  \n",
        "    #We generate the wordcloud using the series created and the mask \n",
        "    word_cloud = WordCloud(width=2000, height=1000, max_font_size=200, background_color=\"black\", max_words=2000, contour_width=1, \n",
        "                           contour_color=\"steelblue\", colormap=\"nipy_spectral\", stopwords=[\"Automation\"])\n",
        "    word_cloud.generate(tweets)\n",
        "    \n",
        "    #wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(tweets_list)\n",
        "    \n",
        "    #Now we plot both figures, the wordcloud and the mask\n",
        "    #plt.figure(figsize=(15,15))\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(word_cloud, interpolation=\"hermite\")\n",
        "    plt.axis(\"off\")\n",
        "    #plt.imshow(avengers_mask, cmap=plt.cm.gray, interpolation=\"bilinear\")\n",
        "    #plt.axis(\"off\")    \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOG5eTRmdxlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def graph(word_frequency, sent):\n",
        "    labels = word_frequency[0][1:51].index\n",
        "    title = \"Word Frequency for %s\" %sent\n",
        "    #Plot the figures\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.bar(np.arange(50), word_frequency[0][1:51], width = 0.8, color = sns.color_palette(\"bwr\"), alpha=0.5, \n",
        "            edgecolor = \"black\", capsize=8, linewidth=1);\n",
        "    plt.xticks(np.arange(50), labels, rotation=90, size=14);\n",
        "    plt.xlabel(\"50 more frequent words\", size=14);\n",
        "    plt.ylabel(\"Frequency\", size=14);\n",
        "    #plt.title('Word Frequency for %s', size=18) %sent;\n",
        "    plt.title(title, size=18)\n",
        "    plt.grid(False);\n",
        "    plt.gca().spines[\"top\"].set_visible(False);\n",
        "    plt.gca().spines[\"right\"].set_visible(False);\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_047snfeAB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "table=cleaning_table(table)  \n",
        "\n",
        "word_frequency = vectorization(table)\n",
        "edges_=[]\n",
        "labels={}\n",
        "G = nx.DiGraph()\n",
        "word_f = sorted(word_frequency, key=lambda d: d['frequency'], reverse=True)[0:100]\n",
        "print(word_f)\n",
        "#give list of top 15 hashtags\n",
        "for ind,item in table.iterrows():\n",
        "    for wrd in word_f:\n",
        "      if wrd['name'] in item['Tweet Text']:  \n",
        "        if isinstance(item['Hashtags'],str):\n",
        "          hashtagsList = item['Hashtags'].split()\n",
        "          for hashtag in hashtagsList:\n",
        "            if((hashtag,wrd['name']) not in edges_):\n",
        "                edges_.append((hashtag,wrd['name']))\n",
        "                labels[wrd['name']]=wrd['name']\n",
        "  #graph(word_frequency, 'all')\n",
        "edge_=set(tuple(i) for i in edges_)\n",
        "  #word_cloud(pd.Series([t for t in table['Tweet Text']]).str.cat(sep=' ')) \n",
        "G.add_edges_from(edge_,weight=1)\n",
        "for node in G.nodes():\n",
        "    size_n.append(5)\n",
        "    n_color.append('blue')  \n",
        "pos = nx.spring_layout(G)\n",
        "plt.figure(figsize=(12,12))\n",
        "_=nx.draw_networkx_nodes(G, pos, node_size=5,node_color='b',alpha = 0.7)\n",
        "_=nx.draw_networkx_edges(G, pos ,alpha=0.2,edge_color='r' )\n",
        "_=nx.draw_networkx_labels(G,pos,labels,font_size=12,font_color='g')  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCN1zmOYczr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# pylint: skip-file\n",
        "import matplotlib\n",
        "import re\n",
        "import numpy\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from collections import *\n",
        "from itertools import count\n",
        "\n",
        "from random import randint\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "class OrderedCounter(Counter, OrderedDict):\n",
        "    pass\n",
        "listOfTweets=[]\n",
        "d =[]\n",
        "G = nx.DiGraph()\n",
        "def get_rt_sources(tweet):\n",
        "    rt_patterns = re.compile(r\"(RT|via)((?:\\b\\W*@\\w+)+)\", re.IGNORECASE)\n",
        "    return [ source.strip()\n",
        "             for tuple in rt_patterns.findall(tweet)\n",
        "                 for source in tuple\n",
        "                     if source not in (\"RT\", \"via\") ]\n",
        "for tweet in table.iterrows():\n",
        "    t=str(tweet[1]['Tweet Text'])\n",
        "    rt_sources = get_rt_sources(t)\n",
        "    if not rt_sources: continue\n",
        "    for rt_source in rt_sources:\n",
        "        l_rt=[]\n",
        "        if \":\" in rt_source:\n",
        "            d=re.split(r'@(\\w+)' ,rt_source)\n",
        "            for rt in d:\n",
        "                if (rt.isalnum() or '_' in rt):\n",
        "                    G.add_edges_from([(tweet[1]['Screen Name'],\"@\"+rt)], weight=tweet[1]['Retweet Count'])\n",
        "                    listOfTweets.append(\"@\"+rt)\n",
        "        else :\n",
        "            G.add_edges_from([(tweet[1]['Screen Name'],rt_source)], weight=tweet[1]['Retweet Count'])\n",
        "            listOfTweets.append(rt_source)\n",
        "counts=Counter(listOfTweets).most_common(30)\n",
        "a=[]\n",
        "size_n=[]\n",
        "labels = {}\n",
        "n_color=[]\n",
        "for i in counts:\n",
        "    d={}\n",
        "    d['node_']=i[0]\n",
        "    d['frequency']=i[1]\n",
        "    a.append(d)\n",
        "for node in G.nodes():\n",
        "    size_n.append(5)\n",
        "    n_color.append('blue')\n",
        "for node in G.nodes():\n",
        "    for idx, item in enumerate(a):\n",
        "        if node in item['node_']:\n",
        "            size_n[idx] = round(item['frequency']/10)\n",
        "            n_color[idx] = 'red'\n",
        "            labels[item['node_']] = item['node_']\n",
        "\n",
        "pos = nx.spring_layout(G)\n",
        "plt.figure(figsize=(12,12))\n",
        "_=nx.draw_networkx_nodes(G, pos, node_size=size_n,node_color=n_color,alpha = 0.7)\n",
        "_=nx.draw_networkx_edges(G, pos ,alpha=0.2,edge_color='r' )\n",
        "_=nx.draw_networkx_labels(G,pos,labels,font_size=12,font_color='g')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}