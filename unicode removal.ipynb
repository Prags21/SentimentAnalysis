{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Welcome To Colaboratory",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prags21/SentimentAnalysis/blob/master/unicode%20removal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0ygD3pubC6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "#Module to handle regular expressions\n",
        "import re\n",
        "#manage files\n",
        "\n",
        "import os\n",
        "#Library for emoji\n",
        "#import emoji\n",
        "#Import pandas and numpy to handle data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#import libraries for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "\n",
        "#Import nltk to check english lexicon\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from nltk.corpus import (\n",
        "    wordnet,\n",
        "    stopwords\n",
        ")\n",
        "\n",
        "#import libraries for tokenization and ML\n",
        "import json;\n",
        "import keras;\n",
        "import keras.preprocessing.text as kpt;\n",
        "#from keras.preprocessing.text import Tokenizer;\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.feature_extraction.text import (\n",
        "    CountVectorizer,\n",
        "    TfidfVectorizer\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "\n",
        "#Import all libraries for creating a deep neural network\n",
        "#Sequential is the standard type of neural network with stackable layers\n",
        "from keras.models import (\n",
        "    Sequential,\n",
        "    model_from_json\n",
        ")\n",
        "#Dense: Standard layers with every node connected, dropout: avoids overfitting\n",
        "from keras.layers import Dense, Dropout, Activation;\n",
        "table=pd.read_csv('Dataset.csv',delimiter = ',',encoding = \"utf-8\")\n",
        "stop = stopwords.words('english')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfB1txJvZN0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kypJGh1nbfKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preprocess text in tweets by removing links, @UserNames, blank spaces, etc.\n",
        "def preprocessing_text(table):\n",
        "    print(table['Tweet Text'][10]) \n",
        "\n",
        "    #put everythin in lowercase\n",
        "    table['Tweet Text'] = table['Tweet Text'].str.lower()\n",
        "  \n",
        "    #removing unicode characters  \n",
        "    for ind,item in enumerate(table['Tweet Text']):\n",
        "        a=''\n",
        "        a= bytes(item, \"utf-8\").decode(\"unicode_escape\")\n",
        "        table.at[ind,'Tweet Text'] = re.sub(r'[^\\x00-\\x7f]',r' ',a) \n",
        "\n",
        "    table['Tweet Text'] = lambda x: re.compile('\\#').sub('', re.compile('RT @').sub('@', x, count=1).strip())\n",
        "\n",
        "    #Replace rt indicating that was a retweet\n",
        "    #table['Tweet Text'] = table['Tweet Text'].str.replace(r'[^rt @\\w+\\W]', '')\n",
        "    print(table['Tweet Text'][10]) \n",
        "\n",
        "    #Replace occurences of mentioning @UserNames\n",
        "    table['Tweet Text'] = table['Tweet Text'].replace(r'@\\w+', '', regex=True)\n",
        "    #Replace links contained in the tweet\n",
        "    table['Tweet Text'] = table['Tweet Text'].replace(r'http\\S+', '', regex=True)\n",
        "    table['Tweet Text'] = table['Tweet Text'].replace(r'www.[^ ]+', '', regex=True)\n",
        "    #remove numbers\n",
        "    table['Tweet Text'] = table['Tweet Text'].replace(r'[0-9]+', '', regex=True)\n",
        "    #replace special characters and puntuation marks\n",
        "    table['Tweet Text'] = table['Tweet Text'].replace(r'[!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~]', '', regex=True)\n",
        "    return table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPxrs_GDfubK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#table=preprocessing_text(table)  \n",
        "#table['Tweet Text'] = table['Tweet Text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "#print(TextBlob(train['Tweet text'][0]).ngrams(2))\n",
        "\n",
        "\n",
        "#print(TextBlob(table['Tweet Text'][1]).words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBqWb5WxcPVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Replace elongated words by identifying those repeated characters and then remove them and compare the new word with the english lexicon\n",
        "def in_dict(word):\n",
        "    if wordnet.synsets(word):\n",
        "        #if the word is in the dictionary, we'll return True\n",
        "        return True\n",
        "\n",
        "def replace_elongated_word(word):\n",
        "    regex = r'(\\w*)(\\w+)\\2(\\w*)'\n",
        "    repl = r'\\1\\2\\3'    \n",
        "    if in_dict(word):\n",
        "        return word\n",
        "    new_word = re.sub(regex, repl, word)\n",
        "    if new_word != word:\n",
        "        return replace_elongated_word(new_word)\n",
        "    else:\n",
        "        return new_word\n",
        "\n",
        "def detect_elongated_words(row):\n",
        "    regexrep = r'(\\w*)(\\w+)(\\2)(\\w*)'\n",
        "    words = [''.join(i) for i in re.findall(regexrep, row)]\n",
        "    for word in words:\n",
        "        if not in_dict(word):\n",
        "            row = re.sub(word, replace_elongated_word(word), row)\n",
        "    return row"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnvesvUOcXcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stop_words(table):\n",
        "    #We need to remove the stop words\n",
        "    stop_words_list = stopwords.words('english')\n",
        "    table['Tweet Text'] = table['Tweet Text'].str.lower()\n",
        "    table['Tweet Text'] = table['Tweet Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words_list)]))\n",
        "    return table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZeCvS80cb3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def replace_antonyms(word):\n",
        "    #We get all the lemma for the word\n",
        "    for syn in wordnet.synsets(word): \n",
        "        for lemma in syn.lemmas(): \n",
        "            #if the lemma is an antonyms of the word\n",
        "            if lemma.antonyms(): \n",
        "                #we return the antonym\n",
        "                return lemma.antonyms()[0].name()\n",
        "    return word\n",
        "            \n",
        "def handling_negation(row):\n",
        "    #Tokenize the row\n",
        "    words = word_tokenize(row)\n",
        "    speach_tags = ['JJ', 'JJR', 'JJS', 'NN', 'VB', 'VBD', 'VBG', 'VBN', 'VBP']\n",
        "    #We obtain the type of words that we have in the text, we use the pos_tag function\n",
        "    tags = nltk.pos_tag(words)\n",
        "    #Now we ask if we found a negation in the words\n",
        "    tags_2 = ''\n",
        "    if \"n't\" in words and \"not\" in words:\n",
        "        tags_2 = tags[min(words.index(\"n't\"), words.index(\"not\")):]\n",
        "        words_2 = words[min(words.index(\"n't\"), words.index(\"not\")):]\n",
        "        words = words[:(min(words.index(\"n't\"), words.index(\"not\")))+1]\n",
        "    elif \"n't\" in words:\n",
        "        tags_2 = tags[words.index(\"n't\"):]\n",
        "        words_2 = words[words.index(\"n't\"):] \n",
        "        words = words[:words.index(\"n't\")+1]\n",
        "    elif \"not\" in words:\n",
        "        tags_2 = tags[words.index(\"not\"):]\n",
        "        words_2 = words[words.index(\"not\"):]\n",
        "        words = words[:words.index(\"not\")+1] \n",
        "        \n",
        "    for index, word_tag in enumerate(tags_2):\n",
        "        if word_tag[1] in speach_tags:\n",
        "            words = words+[replace_antonyms(word_tag[0])]+words_2[index+2:]\n",
        "            break\n",
        "            \n",
        "    return ' '.join(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5ld02CFchFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleaning_table(table):\n",
        "    #This function will process all the required cleaning for the text in our tweets\n",
        "    table = preprocessing_text(table)\n",
        "    table['Tweet Text'] = table['Tweet Text'].apply(lambda x: detect_elongated_words(x))\n",
        "    table['Tweet Text'] = table['Tweet Text'].apply(lambda x: handling_negation(x))\n",
        "    table = stop_words(table)\n",
        "    return table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6WzBWQSc0DP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Vectorization for Data Visualization\n",
        "def vectorization(table):\n",
        "    #CountVectorizer will convert a collection of text documents to a matrix of token counts\n",
        "    #Produces a sparse representation of the counts \n",
        "    #Initialize\n",
        "    vector = CountVectorizer()\n",
        "    #We fit and transform the vector created\n",
        "    frequency_matrix = vector.fit_transform(table['Tweet Text'])\n",
        "    #Sum all the frequencies for each word\n",
        "    sum_frequencies = np.sum(frequency_matrix, axis=0)\n",
        "    #Now we use squeeze to remove single-dimensional entries from the shape of an array that we got from applying np.asarray to\n",
        "    #the sum of frequencies.\n",
        "    frequency = np.squeeze(np.asarray(sum_frequencies))\n",
        "    #Now we get into a dataframe all the frequencies and the words that they correspond to\n",
        "    frequency_df = pd.DataFrame([frequency], columns=vector.get_feature_names())#.transpose()\n",
        "    list_words=[]\n",
        "    for name in frequency_df.columns:\n",
        "      dict={}\n",
        "      dict['name']=name\n",
        "      dict['frequency']=frequency_df[name][0]\n",
        "      list_words.append(dict);\n",
        "    return list_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCjgvfmndF21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def word_cloud(tweets):\n",
        "    \n",
        "    #We get the directory that we are working on\n",
        "    file = os.getcwd()\n",
        "    #We read the mask image into a numpy array\n",
        "    #Now we store the tweets into a series to be able to process \n",
        "    #tweets_list = pd.Series([t for t in tweet_table.tweet]).str.cat(sep=' ')  \n",
        "    #We generate the wordcloud using the series created and the mask \n",
        "    word_cloud = WordCloud(width=2000, height=1000, max_font_size=200, background_color=\"black\", max_words=2000, contour_width=1, \n",
        "                           contour_color=\"steelblue\", colormap=\"nipy_spectral\", stopwords=[\"Automation\"])\n",
        "    word_cloud.generate(tweets)\n",
        "    \n",
        "    #wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(tweets_list)\n",
        "    \n",
        "    #Now we plot both figures, the wordcloud and the mask\n",
        "    #plt.figure(figsize=(15,15))\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(word_cloud, interpolation=\"hermite\")\n",
        "    plt.axis(\"off\")\n",
        "    #plt.imshow(avengers_mask, cmap=plt.cm.gray, interpolation=\"bilinear\")\n",
        "    #plt.axis(\"off\")    \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOG5eTRmdxlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def graph(word_frequency, sent):\n",
        "    labels = word_frequency[0][1:51].index\n",
        "    title = \"Word Frequency for %s\" %sent\n",
        "    #Plot the figures\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.bar(np.arange(50), word_frequency[0][1:51], width = 0.8, color = sns.color_palette(\"bwr\"), alpha=0.5, \n",
        "            edgecolor = \"black\", capsize=8, linewidth=1);\n",
        "    plt.xticks(np.arange(50), labels, rotation=90, size=14);\n",
        "    plt.xlabel(\"50 more frequent words\", size=14);\n",
        "    plt.ylabel(\"Frequency\", size=14);\n",
        "    #plt.title('Word Frequency for %s', size=18) %sent;\n",
        "    plt.title(title, size=18)\n",
        "    plt.grid(False);\n",
        "    plt.gca().spines[\"top\"].set_visible(False);\n",
        "    plt.gca().spines[\"right\"].set_visible(False);\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_047snfeAB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "table=cleaning_table(table)  \n",
        "\n",
        "word_frequency = vectorization(table)\n",
        "edges_=[]\n",
        "labels={}\n",
        "G = nx.DiGraph()\n",
        "word_f = sorted(word_frequency, key=lambda d: d['frequency'], reverse=True)[0:20]\n",
        "print(word_f)\n",
        "for ind,item in table.iterrows():\n",
        "    for wrd in word_f:\n",
        "      if wrd['name'] in item['Tweet Text']:  \n",
        "        if isinstance(item['Hashtags'],str):\n",
        "          hashtagsList = item['Hashtags'].split()\n",
        "          for hashtag in hashtagsList:\n",
        "            edges_.append((hashtag,wrd['name']))\n",
        "          labels[wrd['name']]=wrd['name']\n",
        "  #graph(word_frequency, 'all')\n",
        "\n",
        "  #word_cloud(pd.Series([t for t in table['Tweet Text']]).str.cat(sep=' ')) \n",
        "print(edges_)\n",
        "G.add_edges_from(edges_,weight=1)\n",
        "  \n",
        "pos = nx.spring_layout(G)\n",
        "plt.figure(figsize=(12,12))\n",
        "_=nx.draw_networkx_nodes(G, pos, node_size=5,node_color='b',alpha = 0.7)\n",
        "_=nx.draw_networkx_edges(G, pos ,alpha=0.2,edge_color='r' )\n",
        "_=nx.draw_networkx_labels(G,pos,labels,font_size=12,font_color='g')  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCN1zmOYczr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# pylint: skip-file\n",
        "import matplotlib\n",
        "import re\n",
        "import numpy\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from collections import *\n",
        "from itertools import count\n",
        "\n",
        "from random import randint\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "class OrderedCounter(Counter, OrderedDict):\n",
        "    pass\n",
        "listOfTweets=[]\n",
        "d =[]\n",
        "G = nx.DiGraph()\n",
        "def get_rt_sources(tweet):\n",
        "    rt_patterns = re.compile(r\"(RT|via)((?:\\b\\W*@\\w+)+)\", re.IGNORECASE)\n",
        "    return [ source.strip()\n",
        "             for tuple in rt_patterns.findall(tweet)\n",
        "                 for source in tuple\n",
        "                     if source not in (\"RT\", \"via\") ]\n",
        "for tweet in table.iterrows():\n",
        "    t=str(tweet[1]['Tweet Text'])\n",
        "    rt_sources = get_rt_sources(t)\n",
        "    if not rt_sources: continue\n",
        "    for rt_source in rt_sources:\n",
        "        l_rt=[]\n",
        "        if \":\" in rt_source:\n",
        "            d=re.split(r'@(\\w+)' ,rt_source)\n",
        "            for rt in d:\n",
        "                if (rt.isalnum() or '_' in rt):\n",
        "                    G.add_edges_from([(tweet[1]['Screen Name'],\"@\"+rt)], weight=tweet[1]['Retweet Count'])\n",
        "                    listOfTweets.append(\"@\"+rt)\n",
        "        else :\n",
        "            G.add_edges_from([(tweet[1]['Screen Name'],rt_source)], weight=tweet[1]['Retweet Count'])\n",
        "            listOfTweets.append(rt_source)\n",
        "counts=Counter(listOfTweets).most_common(30)\n",
        "a=[]\n",
        "size_n=[]\n",
        "labels = {}\n",
        "n_color=[]\n",
        "for i in counts:\n",
        "    d={}\n",
        "    d['node_']=i[0]\n",
        "    d['frequency']=i[1]\n",
        "    a.append(d)\n",
        "for node in G.nodes():\n",
        "    size_n.append(5)\n",
        "    n_color.append('blue')\n",
        "for node in G.nodes():\n",
        "    for idx, item in enumerate(a):\n",
        "        if node in item['node_']:\n",
        "            size_n[idx] = round(item['frequency']/10)\n",
        "            n_color[idx] = 'red'\n",
        "            labels[item['node_']] = item['node_']\n",
        "\n",
        "pos = nx.spring_layout(G)\n",
        "plt.figure(figsize=(12,12))\n",
        "_=nx.draw_networkx_nodes(G, pos, node_size=size_n,node_color=n_color,alpha = 0.7)\n",
        "_=nx.draw_networkx_edges(G, pos ,alpha=0.2,edge_color='r' )\n",
        "_=nx.draw_networkx_labels(G,pos,labels,font_size=12,font_color='g')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqL2kUM7tMpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "geolocator = Nominatim(user_agent=\"specify_your_app_name_here\")\n",
        "location = geolocator.geocode(\"175 5th Avenue NYC\")\n",
        "print(location.raw)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}